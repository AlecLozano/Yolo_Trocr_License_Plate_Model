{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Will download the dataset to ur computer\n",
    "path = kagglehub.dataset_download(\"nickyazdani/license-plate-text-recognition-dataset\")\n",
    "image_folder = os.path.join(path, \"cropped_lps/cropped_lps/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "# from PIL import Image, ImageDraw, ImageOps\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# yolo_processor = YolosImageProcessor.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection')\n",
    "# yolo_model = YolosForObjectDetection.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image(row):\n",
    "    image = os.path.join(image_folder, row[\"images\"])\n",
    "    row[\"images\"] = image\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kagglehub import KaggleDatasetAdapter\n",
    "import kagglehub\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lozan\\AppData\\Local\\Temp\\ipykernel_14968\\2659667106.py:9: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  train = kagglehub.load_dataset(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7082a9da640d4a92b15a99e786a4bad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Dataset: Dataset({\n",
      "    features: ['Unnamed: 0', 'images', 'labels'],\n",
      "    num_rows: 6000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loads the csv folder that contains the labels for each image in\n",
    "# the downloaded dataset\n",
    "\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"lpr.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "train = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.HUGGING_FACE,\n",
    "  \"nickyazdani/license-plate-text-recognition-dataset\",\n",
    "  file_path,\n",
    "  pandas_kwargs={'nrows': 6000}\n",
    "  ).map(add_image, batched=False)\n",
    "\n",
    "print(\"Hugging Face Dataset:\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "\n",
    "def crop_objects(ds):\n",
    "    '''\n",
    "     Uses bounding boxes from YOLO model to \n",
    " return list of cropped images that should\n",
    "  contain the license plates\n",
    "    '''\n",
    "\n",
    "    \n",
    "    image = ds[\"image\"]\n",
    "    box = ds[\"bbox\"][0]\n",
    "    \n",
    "    \n",
    "    normalized = all(v < 1 for v in box)  \n",
    "    \n",
    "    x_min, y_min, x_max, y_max = box\n",
    "\n",
    "    if normalized:\n",
    "        width, height = image.size\n",
    "        x_min = int(x_min * width)\n",
    "        y_min = int(y_min * height)\n",
    "        x_max = int(x_max * width)\n",
    "        y_max = int(y_max * height)\n",
    "\n",
    "    # Ensure valid coordinates\n",
    "    if x_min > x_max:\n",
    "        x_min, x_max = x_max, x_min\n",
    "    if y_min > y_max:\n",
    "        y_min, y_max = y_max, y_min\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Crop image\n",
    "        img_cropped = image.crop([x_min, y_min, x_max, y_max])\n",
    "    except:\n",
    "\n",
    "        print(x_min, y_min, x_max, y_max, normalized)\n",
    "        image.show()\n",
    "        raise Exception\n",
    "\n",
    "    ds[\"images\"] = img_cropped\n",
    "    ds[\"labels\"] = ds[\"target\"][0]\n",
    "    ds[\"bbox\"] = box\n",
    "    del ds[\"image\"]\n",
    "    del ds['target']\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "trocr_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed', use_fast=False)\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "augment = transforms.Compose([  # Slight rotation\n",
    "    v2.RandomRotation(10),\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust contrast\n",
    "    v2.GaussianBlur(5),  # Simulate blur\n",
    "    v2.RandomResize(30,300)  # Standardize input size\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LPDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, processor):\n",
    "        self.processor = processor\n",
    "        self.ds = ds\n",
    "        self.pad = 9\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.ds[index]\n",
    "        images = Image.open(row[\"images\"])\n",
    "        images = augment(images)\n",
    "        label = row[\"labels\"]\n",
    "\n",
    "        pixel_values =  self.processor(images, return_tensors=\"pt\").pixel_values.squeeze().to(device)\n",
    "        labels = self.processor.tokenizer(label, return_tensors=\"pt\").input_ids.squeeze().to(device)\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "p_train = LPDataset(train, trocr_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [i[\"input_ids\"] for i in batch]\n",
    "    pixel_values = [i[\"pixel_values\"] for i in batch]\n",
    "\n",
    "    max = 0\n",
    "    for i in input_ids:\n",
    "        if max < i.size()[0]:\n",
    "            max = i.size()[0]\n",
    "    processed = []\n",
    "\n",
    "    for i in input_ids:\n",
    "        processed.append(F.pad(i,(0,max-i.size()[0]), \"constant\", trocr_processor.tokenizer.pad_token_id))\n",
    "    \n",
    "\n",
    "    # for x,b in enumerate(batch):\n",
    "    #     to_add = []\n",
    "    #     for i in range(b[\"input_ids\"].size()[0]):\n",
    "    #         to_add.append(processed.pop(0))\n",
    "    input_ids = torch.stack(processed)\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    return {\"pixel_values\":pixel_values,\"labels\":input_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = DataLoader(p_train, batch_size=20, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "trocr_model.config.decoder_start_token_id = trocr_processor.tokenizer.cls_token_id\n",
    "trocr_model.config.pad_token_id = trocr_processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "trocr_model.config.vocab_size = trocr_model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "trocr_model.config.eos_token_id = trocr_processor.tokenizer.eos_token_id\n",
    "trocr_model.config.max_length = 100\n",
    "trocr_model.config.early_stopping = True\n",
    "trocr_model.config.no_repeat_ngram_size = 3\n",
    "trocr_model.config.length_penalty = 2.0\n",
    "trocr_model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = trocr_processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = trocr_processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370db52964924ded9fa2e9b8b88023ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.6215362014248967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31fb6232c9e47c390af734e0fa611a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.30410739937176307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c54d9d0dfd4008bf2975682445d398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.24423676639795303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6648f4095474031ae0d938729fec6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.2239138032247623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381f21737eed4c75a7be81beba8d008e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 0.21057464459290107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca3eed553f54a6b8e7634c8ace0ab77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 0.18015919050822654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f6ed574c3540f88ce0fb3bc2c12b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 6: 0.1591707134215782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd56b35216646ca8837afb30cfba094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 7: 0.1984381726073722\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b369e7c1acea45808b5511c8ba125021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 8: 0.1605537390196696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc02b861b3354566ab4a2e0f0df9517b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 9: 0.14556533307457964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82040eb81fd1496fb3e19c8f39bb3fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10: 0.1379983901962017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33dc39bc9f14b08b762a7cfdd501670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 11: 0.1300334958328555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb38f2afa15f4009ae366e1546fd5ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 12: 0.11865701586706563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2bdd1296ea4dc582485f58c83dad91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 13: 0.10062769342369089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c954aefd13a44bb88ea7001a13713892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 14: 0.10482839809575428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f4e15ff54740329db231138587df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 15: 0.1231666928405563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b37bad6a2154732b01edc63240c94b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 16: 0.08583858402678743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e8005d4ce34290b20359fa980ffad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 17: 0.08878862353817871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4554c60b143a49e5b6ecf6f6f5fe5d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 18: 0.07735554625823472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d47416fdef411896cf858019b2ae77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 19: 0.07055732684675604\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(trocr_model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(20):\n",
    "    trocr_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(tdl):\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "        outputs = trocr_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss/len(tdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(8,199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m img = \u001b[43mtest\u001b[49m[\u001b[32m10\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m img\n",
      "\u001b[31mNameError\u001b[39m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "img = Image.open('image.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: ['51F4614']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p = trocr_processor(img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    output = trocr_model.generate(p)\n",
    "    results = trocr_processor.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "print(\"Predicted Text:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_model.save_pretrained(\"model/model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
