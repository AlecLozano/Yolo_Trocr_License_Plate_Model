{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"nickyazdani/license-plate-text-recognition-dataset\")\n",
    "image_folder = os.path.join(path, \"cropped_lps/cropped_lps/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "# from PIL import Image, ImageDraw, ImageOps\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# yolo_processor = YolosImageProcessor.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection')\n",
    "# yolo_model = YolosForObjectDetection.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image(row):\n",
    "    image = os.path.join(image_folder, row[\"images\"])\n",
    "    row[\"images\"] = image\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kagglehub import KaggleDatasetAdapter\n",
    "import kagglehub\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lozan\\AppData\\Local\\Temp\\ipykernel_22440\\3524625010.py:9: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  train = kagglehub.load_dataset(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4444e1fe1eb0406bb683df7760e6971d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Dataset: Dataset({\n",
      "    features: ['Unnamed: 0', 'images', 'labels'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[hf-datasets]\n",
    "\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"lpr.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "train = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.HUGGING_FACE,\n",
    "  \"nickyazdani/license-plate-text-recognition-dataset\",\n",
    "  file_path,\n",
    "  pandas_kwargs={'nrows': 1000}\n",
    "  ).map(add_image, batched=False)\n",
    "\n",
    "print(\"Hugging Face Dataset:\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "# image = train[4][\"image\"].copy()\n",
    "\n",
    "# inputs = yolo_processor.preprocess(images=image, return_tensors=\"pt\").to(device)\n",
    "# outputs = yolo_model(**inputs)\n",
    "\n",
    "# # model predicts bounding boxes and corresponding face mask detection classes\n",
    "# logits = outputs.logits\n",
    "# bboxes = outputs.pred_boxes\n",
    "# results = yolo_processor.post_process_object_detection(outputs=outputs, threshold=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "\n",
    "def crop_objects(ds):\n",
    "    padding_to = 400\n",
    "    image = ds[\"image\"]\n",
    "    box = ds[\"bbox\"][0]\n",
    "    \n",
    "    \n",
    "    normalized = all(v < 1 for v in box)  \n",
    "    \n",
    "    x_min, y_min, x_max, y_max = box\n",
    "\n",
    "    if normalized:\n",
    "        width, height = image.size\n",
    "        x_min = int(x_min * width)\n",
    "        y_min = int(y_min * height)\n",
    "        x_max = int(x_max * width)\n",
    "        y_max = int(y_max * height)\n",
    "\n",
    "    # Ensure valid coordinates\n",
    "    if x_min > x_max:\n",
    "        x_min, x_max = x_max, x_min\n",
    "    if y_min > y_max:\n",
    "        y_min, y_max = y_max, y_min\n",
    "\n",
    "    # Crop image\n",
    "    try:\n",
    "        img_cropped = image.crop([x_min, y_min, x_max, y_max])\n",
    "    except:\n",
    "        print(x_min, y_min, x_max, y_max, normalized)\n",
    "        print(box)\n",
    "        image.show()\n",
    "        raise Exception\n",
    "\n",
    "    ds[\"images\"] = img_cropped\n",
    "    ds[\"labels\"] = ds[\"target\"][0]\n",
    "    ds[\"bbox\"] = box\n",
    "    del ds[\"image\"]\n",
    "    del ds['target']\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "test = load_dataset(\"sonnetechnology/license-plate-text-recognition-full\", split=\"test[:300]\").map(crop_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "trocr_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed', use_fast=False)\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "augment = transforms.Compose([  # Slight rotation\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust contrast\n",
    "    v2.GaussianBlur(5),  # Simulate blur\n",
    "    v2.RandomResize(30,300)  # Standardize input size\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test[0][\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LPDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, processor):\n",
    "        self.processor = processor\n",
    "        self.ds = ds\n",
    "        self.pad = 9\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.ds[index]\n",
    "        images = Image.open(row[\"images\"])\n",
    "        images = augment(images)\n",
    "        label = row[\"labels\"]\n",
    "\n",
    "        pixel_values =  self.processor(images, return_tensors=\"pt\").pixel_values.squeeze().to(device)\n",
    "        labels = self.processor.tokenizer(label, return_tensors=\"pt\").input_ids.squeeze().to(device)\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "p_train = LPDataset(train, trocr_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [i[\"input_ids\"] for i in batch]\n",
    "    pixel_values = [i[\"pixel_values\"] for i in batch]\n",
    "\n",
    "    max = 0\n",
    "    for i in input_ids:\n",
    "        if max < i.size()[0]:\n",
    "            max = i.size()[0]\n",
    "    processed = []\n",
    "\n",
    "    for i in input_ids:\n",
    "        processed.append(F.pad(i,(0,max-i.size()[0]), \"constant\", trocr_processor.tokenizer.pad_token_id))\n",
    "    \n",
    "\n",
    "    # for x,b in enumerate(batch):\n",
    "    #     to_add = []\n",
    "    #     for i in range(b[\"input_ids\"].size()[0]):\n",
    "    #         to_add.append(processed.pop(0))\n",
    "    input_ids = torch.stack(processed)\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    return {\"pixel_values\":pixel_values,\"labels\":input_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = DataLoader(p_train, batch_size=20, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "trocr_model.config.decoder_start_token_id = trocr_processor.tokenizer.cls_token_id\n",
    "trocr_model.config.pad_token_id = trocr_processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "trocr_model.config.vocab_size = trocr_model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "trocr_model.config.eos_token_id = trocr_processor.tokenizer.eos_token_id\n",
    "trocr_model.config.max_length = 100\n",
    "trocr_model.config.early_stopping = True\n",
    "trocr_model.config.no_repeat_ngram_size = 3\n",
    "trocr_model.config.length_penalty = 2.0\n",
    "trocr_model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = trocr_processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = trocr_processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc4028324794f8f88bca1aea501aee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 3.6550453126430513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28668e6586e4a659fb4054d90e2728a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.8374545037746429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d15cf33a31b44f8aa36ab5ee4359922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.639370573759079\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec247adf6fb4f05839bb27dad977468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.5795382153987885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fae673408a485aab68b4bfb5e8cc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 0.5284843641519547\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af4723b090e461f932d1845e44608dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 0.5588742631673813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd014c410c04ee8aa1e8ac8428d32da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 6: 0.4687989765405655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910a52fbaf134932a16f22c13fe81420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 7: 0.43418241560459137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d711350092d41448a96e37b4ccbbdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 8: 0.4443281239271164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a26b5c9da8245d5acb0e7d4304957e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 9: 0.40024952113628387\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(trocr_model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    trocr_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(tdl):\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "        outputs = trocr_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss/len(tdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(8,199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAdAGgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwtbK4li85IHMOcb24FOXTbpiMQgZ9WGPzr0PwVIV8D61LGqG5t7RponZAxQjzORkEdq3tAvNK1K7sNvjO8l1BlSRrVQkSu5GShwgB54xmgR5TB4e1K4I8u3U55wOePwq/B4K1ycnyrVzjrtgkz+q16v4gk1C18TeHLuPUp47Wa+it2tE+RPm3ZJx1+hqHxm8P9seGodQkuxpzvcrOluXy/wAq7RtTk8+nbNAXPP4fh1rsjfNbXQ9T5Kj/ANCYVpJ8L7tInlvPOhRAWZ5Z40AAGSTjdjiu+0K08NaVZXOqaXby2cJUiaW584Havchyf5VD4r1OK+8Hqmk3KTNq0y2VuyZw28kNx7AGgRzFl8Lra4t4popbSaGRQySvcyOrKe42qtaOk+BNFmvby1hktGlsmVLjFmzqrMM4DOxBPrXb2c+m2+mCW1uIG0+zQx70YbVEY5H4AVgeCL21jtYrS6m26tqu/V5YipwUkfC89M4A49zQFzCmtdMs7++g0zStSvlsDi8lsYYIghA5UHaCxHoK6rTtC0e+sre7iN9PFPGJI0uLpxkEZwRxg9K57TNEl1GbxdcrrWqWTw6nOUjtLjy1DAAl2A656fhXXeGr+71TwxpN/dNvuJbZZJGwBk5xnp3xQDZheFoftuoa5b6nomkxSWM0SosMO4gMpblm5PGK0pbm4m8dWGm2Lm0trO1e8vBCu1XLfLGpP5n8Kh8PMP8AhL/GKDki5tT/AOQjXO6o3iuw1q7u7fTpIjf6xBFFdLcDDQplUQqoLBDnJPFIRP8AFaVn0+23HIW3nP4/IP60VW+J73J0+xW7iiW6e0maRISWQHKZwT2opIox/hiFuLDUrJ0MqTWzRvGrbSwyQVz2JDda1r3Ure/8OaVoen6XeQapbXEOy0kgcm32OCW8wgAjHfqc9K800PxLeaCJDaErJJkbweRntyCO1aMvxG8SSLsF+6jvjAz+QFUB6x4xtppjokltDI/ka1BIwRCdqZbJPFTa9ZXc3iLw1dW1rJKtvfN5rKn3UYYyTXijeMvEL5J1W5GQQcStz+tV5PEmsy5Lajcc8/fNIVj6PbfjMnv1PHPXk1h69p2navPpz3ettZPYyGSM29wiEMQME5z0wPzNeCPf3r8vdSNn1NRs8j8tNIR6bqY7HsPh7T9C0LTNQstW8QLcfazJG0KXbFGiYjkgD72B1HrVQJ4A0XXY9RhuDLFHAqRWxjeTbIGDB1Zz6DGM15WIzISrySMAuRlqiYIq52ZHp/8AXxQwPTtT8TeDbi9ubhG1eNbpt11Db3RjinOBncobnPeiT4m6HaXsN3p+jSCSC2+ywjz2VUQcgBRgZ968udgo+VR+PNIzsSOcew6Uh2PRH+Kd/HLcT2WmWtvJcMGkkWEBpCAQpZiTnANZt18SfENyMi6EQ44VgMen3cGuMQbjk9alyqMeCfl9h3HtQBpah4i1HVLdYLqYSgAgNs+Yg9tx+gorLRmlB5Ax6qDRTsB//9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=104x29>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = test[10][\"images\"]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: ['51F4614']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p = trocr_processor(img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    output = trocr_model.generate(p)\n",
    "    results = trocr_processor.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "print(\"Predicted Text:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_model.save_pretrained(\"model/model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
