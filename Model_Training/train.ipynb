{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Will download the dataset to ur computer\n",
    "path = kagglehub.dataset_download(\"nickyazdani/license-plate-text-recognition-dataset\")\n",
    "image_folder = os.path.join(path, \"cropped_lps/cropped_lps/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "# from PIL import Image, ImageDraw, ImageOps\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# yolo_processor = YolosImageProcessor.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection')\n",
    "# yolo_model = YolosForObjectDetection.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image(row):\n",
    "    image = os.path.join(image_folder, row[\"images\"])\n",
    "    row[\"images\"] = image\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kagglehub import KaggleDatasetAdapter\n",
    "import kagglehub\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lozan\\AppData\\Local\\Temp\\ipykernel_14968\\2659667106.py:9: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  train = kagglehub.load_dataset(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7082a9da640d4a92b15a99e786a4bad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Dataset: Dataset({\n",
      "    features: ['Unnamed: 0', 'images', 'labels'],\n",
      "    num_rows: 6000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loads the csv folder that contains the labels for each image in\n",
    "# the downloaded dataset\n",
    "\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"lpr.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "train = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.HUGGING_FACE,\n",
    "  \"nickyazdani/license-plate-text-recognition-dataset\",\n",
    "  file_path,\n",
    "  pandas_kwargs={'nrows': 6000}\n",
    "  ).map(add_image, batched=False)\n",
    "\n",
    "print(\"Hugging Face Dataset:\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "\n",
    "def crop_objects(ds):\n",
    "    '''\n",
    "     Uses bounding boxes from YOLO model to \n",
    " return list of cropped images that should\n",
    "  contain the license plates\n",
    "    '''\n",
    "\n",
    "    \n",
    "    image = ds[\"image\"]\n",
    "    box = ds[\"bbox\"][0]\n",
    "    \n",
    "    \n",
    "    normalized = all(v < 1 for v in box)  \n",
    "    \n",
    "    x_min, y_min, x_max, y_max = box\n",
    "\n",
    "    if normalized:\n",
    "        width, height = image.size\n",
    "        x_min = int(x_min * width)\n",
    "        y_min = int(y_min * height)\n",
    "        x_max = int(x_max * width)\n",
    "        y_max = int(y_max * height)\n",
    "\n",
    "    # Ensure valid coordinates\n",
    "    if x_min > x_max:\n",
    "        x_min, x_max = x_max, x_min\n",
    "    if y_min > y_max:\n",
    "        y_min, y_max = y_max, y_min\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Crop image\n",
    "        img_cropped = image.crop([x_min, y_min, x_max, y_max])\n",
    "    except:\n",
    "\n",
    "        print(x_min, y_min, x_max, y_max, normalized)\n",
    "        image.show()\n",
    "        raise Exception\n",
    "\n",
    "    ds[\"images\"] = img_cropped\n",
    "    ds[\"labels\"] = ds[\"target\"][0]\n",
    "    ds[\"bbox\"] = box\n",
    "    del ds[\"image\"]\n",
    "    del ds['target']\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "trocr_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed', use_fast=False)\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "augment = transforms.Compose([  # Slight rotation\n",
    "    v2.RandomRotation(10),\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust contrast\n",
    "    v2.GaussianBlur(5),  # Simulate blur\n",
    "    v2.RandomResize(30,300)  # Standardize input size\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LPDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, processor):\n",
    "        self.processor = processor\n",
    "        self.ds = ds\n",
    "        self.pad = 9\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.ds[index]\n",
    "        images = Image.open(row[\"images\"])\n",
    "        images = augment(images)\n",
    "        label = row[\"labels\"]\n",
    "\n",
    "        pixel_values =  self.processor(images, return_tensors=\"pt\").pixel_values.squeeze().to(device)\n",
    "        labels = self.processor.tokenizer(label, return_tensors=\"pt\").input_ids.squeeze().to(device)\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "p_train = LPDataset(train, trocr_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [i[\"input_ids\"] for i in batch]\n",
    "    pixel_values = [i[\"pixel_values\"] for i in batch]\n",
    "\n",
    "    max = 0\n",
    "    for i in input_ids:\n",
    "        if max < i.size()[0]:\n",
    "            max = i.size()[0]\n",
    "    processed = []\n",
    "\n",
    "    for i in input_ids:\n",
    "        processed.append(F.pad(i,(0,max-i.size()[0]), \"constant\", trocr_processor.tokenizer.pad_token_id))\n",
    "    \n",
    "\n",
    "    # for x,b in enumerate(batch):\n",
    "    #     to_add = []\n",
    "    #     for i in range(b[\"input_ids\"].size()[0]):\n",
    "    #         to_add.append(processed.pop(0))\n",
    "    input_ids = torch.stack(processed)\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    return {\"pixel_values\":pixel_values,\"labels\":input_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = DataLoader(p_train, batch_size=20, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "trocr_model.config.decoder_start_token_id = trocr_processor.tokenizer.cls_token_id\n",
    "trocr_model.config.pad_token_id = trocr_processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "trocr_model.config.vocab_size = trocr_model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "trocr_model.config.eos_token_id = trocr_processor.tokenizer.eos_token_id\n",
    "trocr_model.config.max_length = 100\n",
    "trocr_model.config.early_stopping = True\n",
    "trocr_model.config.no_repeat_ngram_size = 3\n",
    "trocr_model.config.length_penalty = 2.0\n",
    "trocr_model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = trocr_processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = trocr_processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(trocr_model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(20):\n",
    "    trocr_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(tdl):\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "        outputs = trocr_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss/len(tdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(8,199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAArAFUDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDTC5NAGDW3ouhf2lGZGkwAwFbNz4StraHzQzyY6qO9bcyMeVnGAENinEZ6V1NrYW5fY2nSADqxq/daXDAYzbWysp6knOKXONQZw6xO33VJ+gpyQTEkLG5OfSu/s100Run7pZPTI61kvCthqQup7pFhzkLxzU+0K9mjnfsN03/LF8/Spl0O+lUMsXB7musmvbLWFC2d0om6YwKzrvVBo0Ys5omuHc4wnanzgoXMcaFNuCtLErehNaUHhK52lmmQAjIIq1pyQXU4f7BKme7k8VNdeKI7XVU01V5PFJyBUzl9SsJdPujDJhjjINFW/E8xfVcjj5RxRTuQ1YveE7mVYrhIcGQchTS6ZreoyeIGtL0jaW+UCqvhaYRant/vVLqbpa+MLdwNpJ5HrUSTNI6o2fF2oy2emlYSRIwxkda5fSNQu20WVZpn80A5yaveJdSR9XijlV/KYdQM4rKtrmMLMkUbENx04NQy7E3laWNPS5W4Y3efm+bvVexkF1qUEV1ucB+MngirFraadb6NJJIqm5DcDNU9PWSzu1uZ0kEW/K/KaZNnc1ruODS9eR4kITdkqtVdR1Bn16OWCF3IbIU1oxzjWvECSW8LGNOSSMVYubOVfEcVx5WyEHk44quhV11OjtL2SSyM01u8JCZ5FeYXZlk8R/bgVChiQM9a9H1K+t5rSSGCeMysMKPQ1yY8EzNbCeSYiYncRnik0wVkZ2rTG4uxJ6qKKhu4DBcNEzbivGaK1SMW0XdJlFvqcL5AGRmuu1XSINSvbe9+0IuznriuE9D3FSxXM7R4MrEfWpkgjKx2V/qUaOAbOGQKMbiKjs/7FjQzSbUduStcoHZlwWJ59aHAyPpQolOTOkkn8PLKX8rcT6VPJ4l0vyxF9kyo6ZWuQbrTT1quVMXOzpx4qihb9zZIvuBiq8/i25mBXyU2+hFYDdKjp8iE5Nlp9QkM/mxoqt7VNNq99KoBnIAGOKzu9PPaqUURzMhcs7lmyxPc0U8/eNFURc//2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=85x43>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('image.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: ['G5875']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p = trocr_processor(img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    output = trocr_model.generate(p)\n",
    "    results = trocr_processor.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "print(\"Predicted Text:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lozan\\OneDrive - East Carolina University\\ECU Materials\\Sophomore\\Spring\\Natural Language Processing\\HW\\Yolo\\Yolo_Trocr_License_Plate_Model\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 100, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trocr_model.save_pretrained(\"model/model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
